{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e044b5-40cf-4f77-ba2f-9aaca33f1160",
   "metadata": {},
   "source": [
    "# Machine learning and statistical modeling questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45661aef-bc7a-413b-8ad1-0e6b9ee42f5c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Interview Questions\n",
    "\n",
    "## 1. Linear Regression: Multicollinearity\n",
    "**Question**: In a linear regression model, what happens if two or more features are highly collinear? How would you detect and handle this in practice?\n",
    "\n",
    "**What it tests**: \n",
    "- Understanding of **multicollinearity** and its impact on linear models.\n",
    "- Ability to explain concepts like **variance inflation factor (VIF)**.\n",
    "- Knowing practical techniques to address multicollinearity, such as **feature selection**, **regularization (Ridge/Lasso)**, or **principal component analysis (PCA)**.\n",
    "\n",
    "**Follow-up**: What would be the effect of multicollinearity on the interpretability of your model’s coefficients?\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decision Trees: Handling Imbalanced Data\n",
    "**Question**: How do decision trees handle imbalanced data? What modifications would you make to improve performance on imbalanced datasets?\n",
    "\n",
    "**What it tests**: \n",
    "- Knowledge of **how decision trees split data** and how they can become biased towards the majority class.\n",
    "- Understanding of techniques like **class weighting**, **sampling (SMOTE/undersampling)**, or **modifying split criteria** (e.g., using **Gini impurity** or **entropy**).\n",
    "- The ability to discuss **evaluation metrics** that go beyond accuracy (e.g., **precision, recall, F1-score, AUC**).\n",
    "\n",
    "**Follow-up**: What challenges might arise when using these techniques, and how would you monitor model performance?\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Random Forest: Out-of-Bag Error and Feature Importance\n",
    "**Question**: In a random forest, how does the **out-of-bag (OOB) error** work, and how would you interpret it? How does the algorithm calculate feature importance?\n",
    "\n",
    "**What it tests**:\n",
    "- Understanding of **ensemble methods** and the **bootstrap sampling** used in random forests.\n",
    "- Knowledge of **OOB error** as an internal validation technique and its interpretation.\n",
    "- A candidate’s ability to explain **Gini importance** or **permutation feature importance** and when/why these measures could be misleading.\n",
    "\n",
    "**Follow-up**: How would you handle a situation where OOB error significantly underestimates/overestimates test error?\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Logistic Regression: Convergence Issues\n",
    "**Question**: Logistic regression is failing to converge during training. What are some possible reasons, and how would you go about diagnosing and solving the issue?\n",
    "\n",
    "**What it tests**:\n",
    "- Knowledge of **optimization algorithms** used in logistic regression (e.g., **Newton-Raphson**, **gradient descent**).\n",
    "- Ability to explain potential causes of convergence problems, such as **perfect separation**, **poor scaling of features**, or **too many iterations**.\n",
    "- Experience with practical techniques to solve these issues, like **regularization**, **feature scaling**, or changing the optimization algorithm.\n",
    "\n",
    "**Follow-up**: If adding regularization doesn’t solve the issue, what would you try next?\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Support Vector Machines (SVM): Non-linearly Separable Data\n",
    "**Question**: In SVM, what happens when the data is not linearly separable? How would you modify the model to handle such a case?\n",
    "\n",
    "**What it tests**:\n",
    "- Knowledge of **soft-margin SVM** and how the **C parameter** controls the trade-off between margin size and classification error.\n",
    "- Understanding of how the **kernel trick** allows SVM to handle non-linear data by mapping it to higher dimensions.\n",
    "- Awareness of **kernel choice** (linear, polynomial, RBF) and its impact on model performance.\n",
    "\n",
    "**Follow-up**: What are the computational trade-offs of using non-linear kernels, and how would you mitigate them in large datasets?\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gradient Boosting: Overfitting and Hyperparameter Tuning\n",
    "**Question**: Gradient boosting models are prone to overfitting. What are some hyperparameters you can tune to reduce overfitting, and how do they work?\n",
    "\n",
    "**What it tests**:\n",
    "- Understanding of **boosting** and why **overfitting** happens due to sequential learning on residuals.\n",
    "- Knowledge of key hyperparameters like **learning rate**, **number of trees**, **tree depth**, and **min_samples_split**.\n",
    "- Familiarity with **regularization techniques** specific to boosting models, such as **shrinkage**, **subsampling**, or **early stopping**.\n",
    "\n",
    "**Follow-up**: How would you balance between reducing overfitting and maintaining model performance? What role does cross-validation play here?\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Neural Networks: Vanishing Gradient Problem\n",
    "**Question**: Explain the **vanishing gradient problem** in deep neural networks. How would you address this when training very deep networks?\n",
    "\n",
    "**What it tests**:\n",
    "- Knowledge of the **backpropagation algorithm** and how **gradients** propagate through the layers.\n",
    "- Ability to explain the root causes of vanishing gradients, especially with activation functions like **sigmoid** or **tanh**.\n",
    "- Familiarity with solutions such as **ReLU activation**, **batch normalization**, **Xavier/He initialization**, or using **residual connections** (ResNet).\n",
    "\n",
    "**Follow-up**: How would you monitor training to detect vanishing/exploding gradient issues?\n",
    "\n",
    "---\n",
    "\n",
    "## 8. PCA: Dealing with Outliers\n",
    "**Question**: How does **Principal Component Analysis (PCA)** handle outliers? What impact do they have on the resulting components, and how would you mitigate this?\n",
    "\n",
    "**What it tests**:\n",
    "- Understanding of how **PCA maximizes variance**, which means outliers can disproportionately affect the direction of the principal components.\n",
    "- Knowledge of techniques like **robust PCA**, **outlier detection**, or using **dimensionality reduction methods** that are less sensitive to outliers, such as **t-SNE** or **UMAP**.\n",
    "\n",
    "**Follow-up**: If removing outliers is not an option, what would you do to minimize their impact?\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Time Series: Autocorrelation and Stationarity\n",
    "**Question**: In time series modeling, how do you check for **stationarity**, and why is it important? How would you handle a non-stationary time series?\n",
    "\n",
    "**What it tests**:\n",
    "- Understanding of the importance of **stationarity** for models like **ARIMA** and how to check it using **ACF/PACF plots** or tests like **Augmented Dickey-Fuller (ADF)**.\n",
    "- Ability to explain how non-stationary data leads to spurious correlations and why it affects model performance.\n",
    "- Experience with techniques to handle non-stationarity, such as **differencing**, **transformations**, or **including trend and seasonality components**.\n",
    "\n",
    "**Follow-up**: How would you address seasonality in time series data?\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Regularization: Lasso vs. Ridge\n",
    "**Question**: What’s the key difference between **Lasso** and **Ridge** regression? In what situations would you prefer one over the other?\n",
    "\n",
    "**What it tests**:\n",
    "- Knowledge of how **Lasso** (L1 regularization) and **Ridge** (L2 regularization) differ mathematically and practically.\n",
    "- Ability to explain how **Lasso** encourages **sparsity** (feature selection) by shrinking coefficients to zero, while **Ridge** shrinks all coefficients but never fully eliminates them.\n",
    "- Understanding of which situations benefit from **Lasso** (e.g., when you expect many irrelevant features) vs. **Ridge** (e.g., when all features are useful but need regularization).\n",
    "\n",
    "**Follow-up**: What would happen if you combine both methods (Elastic Net), and when would that be useful?\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Clustering: Dealing with Non-Spherical Clusters\n",
    "**Question**: How does K-Means handle non-spherical clusters, and what alternative clustering methods would you use in such situations?\n",
    "\n",
    "**What it tests**:\n",
    "- Understanding of how **K-Means** assumes clusters are spherical and uses Euclidean distance, which can fail for non-spherical, elongated clusters.\n",
    "- Knowledge of alternative methods like **DBSCAN** or **Gaussian Mixture Models (GMMs)**, which handle more complex cluster shapes and densities.\n",
    "\n",
    "**Follow-up**: How would you evaluate the performance of a clustering algorithm when you don’t have ground truth labels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2370ac-da19-414e-9bec-945980f9324c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
